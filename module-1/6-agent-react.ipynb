{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a44f010",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-1/agent.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239232-lesson-6-agent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98f5e36a-da49-4ae2-8c74-b910a2f992fc",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "## Review\n",
    "\n",
    "We built a router.\n",
    "\n",
    "* Our chat model will decide to make a tool call or not based upon the user input\n",
    "* We use a conditional edge to route to a node that will call our tool or simply end\n",
    "\n",
    "![Screenshot 2024-08-21 at 12.44.33 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbac0ba0bd34b541c448cc_agent1.png)\n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, we can extend this into a generic agent architecture.\n",
    "\n",
    "In the above router, we invoked the model and, if it chose to call a tool, we returned a `ToolMessage` to the user.\n",
    " \n",
    "But, what if we simply pass that `ToolMessage` *back to the model*?\n",
    "\n",
    "We can let it either (1) call another tool or (2) respond directly.\n",
    "\n",
    "This is the intuition behind [ReAct](https://react-lm.github.io/), a general agent architecture.\n",
    "  \n",
    "* `act` - let the model call specific tools \n",
    "* `observe` - pass the tool output back to the model \n",
    "* `reason` - let the model reason about the tool output to decide what to do next (e.g., call another tool or just respond directly)\n",
    "\n",
    "This [general purpose architecture](https://blog.langchain.dev/planning-for-agents/) can be applied to many types of tools. \n",
    "\n",
    "![Screenshot 2024-08-21 at 12.45.43 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbac0b4a2c1e5e02f3e78b_agent2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63edff5a-724b-474d-9db8-37f0ae936c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain_openai langchain_core langgraph langchain_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71795ff1-d6a7-448d-8b55-88bbd1ed3dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "import random\n",
    "from typing import Union\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "# This will be a tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "def return_random_number(x: Union[int, str]) -> float:\n",
    "    \"\"\"Multiply x by a random number\n",
    "       between 0 and 1.\n",
    "\n",
    "    Args:\n",
    "        x: first int\n",
    "    \"\"\"\n",
    "    return int(x) * random.random()\n",
    "\n",
    "tools = [add, multiply, divide]\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:8b-instruct-q8_0\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "llm_steps_checker = ChatOllama(\n",
    "    model=\"llama3.1:8b-instruct-q8_0\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# For this ipynb we set parallel tool calling to false as math generally is done sequentially, and this time we have 3 tools that can do math\n",
    "# the OpenAI model specifically defaults to parallel tool calling for efficiency, see https://python.langchain.com/docs/how_to/tool_calling_parallel/\n",
    "# play around with it and see how the model behaves with math equations!\n",
    "\n",
    "#llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cec014-3023-405c-be79-de8fc7adb346",
   "metadata": {},
   "source": [
    "Let's create our LLM and prompt it with the overall desired agent behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d061813f-ebc0-432c-91ec-3b42b15c30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# System message\n",
    "sys_msg_assistant = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    "                        \" The tools need to be used sequentially. If you don't have the previous result, calculate\"\n",
    "                        \" that result first. You will be called back with the previous value, so don't worry about\"\n",
    "                        \" doing multiple calculations at once, which is completely wrong!\")\n",
    "\n",
    "# Node\n",
    "def assistant(state: MessagesState):\n",
    "   print(\"=> Entering assistant node\") # Debugging print\n",
    "   print(\"=> Assistant State:\", state[\"messages\"], state.get(\"tasks\")) # Debugging print\n",
    "   result = llm_with_tools.invoke([sys_msg_assistant] + state[\"messages\"], stream=False)\n",
    "   print(\"=> Assistant Result:\", result) # Debugging print\n",
    "   return {\"messages\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2086f9d0-2d7c-4909-84a9-7bcec98129ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatOllama(model='llama3.1:8b-instruct-q8_0', temperature=0.0), kwargs={'format': 'json', 'structured_output_format': {'kwargs': {'method': 'json_mode'}, 'schema': typing.List[__main__.Task]}}, config={}, config_factories=[])\n",
       "| JsonOutputParser()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_msg_steps_manager = SystemMessage(content=\"You are a helpful assistant that splits the tasks in steps. \"\n",
    "                                              \"But do not resolve the tasks.\")\n",
    "\n",
    "def steps_manager(state: MessagesState):\n",
    "   print(\"=> Entering steps_manager node\") # Debugging print\n",
    "   print(\"=> Steps Manager State:\", state[\"messages\"]) # Debugging print\n",
    "   aimessage = llm_steps_checker.invoke([sys_msg_steps_manager] + state[\"messages\"], stream=False)\n",
    "   print(\"=> Steps Manager Result:\", aimessage) # Debugging print\n",
    "   humman_message = HumanMessage(content=\"Process each task individually\")\n",
    "   return MessagesState({\"messages\": state[\"messages\"] + [aimessage] + [humman_message]})\n",
    "\n",
    "\"\"\"\n",
    "https://medium.com/@gitmaxd/understanding-state-in-langgraph-a-comprehensive-guide-191462220997\n",
    "def add_message(state: ComplexState, message: str, is_human: bool = True) -> ComplexState:\n",
    "    new_message = HumanMessage(content=message) if is_human else AIMessage(content=message)\n",
    "    return ComplexState(\n",
    "        count=state[\"count\"],\n",
    "        messages=state[\"messages\"] + [new_message]\n",
    "    )\n",
    "\"\"\"\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List  # Import List from typing\n",
    "\n",
    "class Task(BaseModel):\n",
    "    task_id: str = Field(description=\"Task id\")\n",
    "    task_message: str = Field(description=\"The task\")\n",
    "\n",
    "\n",
    "llm_steps_checker.with_structured_output(List[Task], method=\"json_mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb43343-9a6f-42cb-86e6-4380f928633c",
   "metadata": {},
   "source": [
    "As before, we use `MessagesState` and define a `Tools` node with our list of tools.\n",
    "\n",
    "The `Assistant` node is just our model with bound tools.\n",
    "\n",
    "We create a graph with `Assistant` and `Tools` nodes.\n",
    "\n",
    "We add `tools_condition` edge, which routes to `End` or to `Tools` based on  whether the `Assistant` calls a tool.\n",
    "\n",
    "Now, we add one new step:\n",
    "\n",
    "We connect the `Tools` node *back* to the `Assistant`, forming a loop.\n",
    "\n",
    "* After the `assistant` node executes, `tools_condition` checks if the model's output is a tool call.\n",
    "* If it is a tool call, the flow is directed to the `tools` node.\n",
    "* The `tools` node connects back to `assistant`.\n",
    "* This loop continues as long as the model decides to call tools.\n",
    "* If the model response is not a tool call, the flow is directed to END, terminating the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d7b71d-85ce-4a70-a785-33e0144169bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='To solve this problem, we need to follow the order of operations:\\n\\n1. Add 3 and 4:\\n   3 + 4 = 7\\n\\n2. Multiply the output (7) by 2:\\n   7 ร 2 = 14\\n\\n3. Divide the output (14) by 5:\\n   14 รท 5 = 2.8', additional_kwargs={}, response_metadata={'model': 'llama3.1:8b-instruct-q8_0', 'created_at': '2025-02-19T09:09:13.156589053Z', 'done': True, 'done_reason': 'stop', 'total_duration': 29998988592, 'load_duration': 6221902267, 'prompt_eval_count': 31, 'prompt_eval_duration': 1472000000, 'eval_count': 80, 'eval_duration': 22301000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-214f7815-c5be-477a-a4aa-edbabdd22c59-0', usage_metadata={'input_tokens': 31, 'output_tokens': 80, 'total_tokens': 111})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Add 3 and 4. Multiply the output by 2. Divide the output by 5.\")]\n",
    "llm_steps_checker.invoke(\"Add 3 and 4. Multiply the output by 2. Divide the output by 5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "659c7a95-b728-414d-8f92-3879b5af2840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task(id='1', message='Add 3 and 4.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "# https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/pydantic/\n",
    "class Task(BaseModel):\n",
    "    id: str = Field(description=\"The task id\")\n",
    "    message: str = Field(description=\"The task message\")\n",
    "\n",
    "\n",
    "message = AIMessage(content='{\"id\": \"1\", \"message\": \"Add 3 and 4.\"}')\n",
    "parser = PydanticOutputParser(pydantic_object=Task)\n",
    "parser.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13a06a8b-ff31-4557-a9d1-1dc5119ecfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tasks(queue=[Task(task_id='1', description='Add 3.'), Task(task_id='2', description='Add 4.'), Task(task_id='3', description='Multiply the output by 2.'), Task(task_id='4', description='Divide the output by 5.')], status='SUCCESS')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Task(BaseModel):\n",
    "    task_id: str = Field(description=\"The task id\")\n",
    "    description: str = Field(description=\"The task message\")\n",
    "\n",
    "\n",
    "class Tasks(BaseModel):\n",
    "    queue: List[Task]\n",
    "    status: str = Field(description=\"Field to set the status 'SUCCESS' or 'FAILURE'\")\n",
    "                        \n",
    "\"\"\"\n",
    "class Tasks(BaseModel):\n",
    "    tasks: List[str]\n",
    "\"\"\"  \n",
    "test_structured_llm = ChatOllama(\n",
    "    model=\"llama3.1:8b-instruct-q8_0\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")\n",
    "PROMPT = \"\"\"\n",
    "You are an AI designed to break down a given problem into a structured list of tasks. Each task should be clearly defined and sequentially dependent on the previous one, but you should not solve the problem.\n",
    "\n",
    "Return a JSON array where each element is an object representing a single step in the process. If you cannot infer any tasks, set the \"status\" field to 'FAILURE'.\n",
    "\n",
    "Example instruction:\n",
    "    \"Take the mug out, add the tea bag and pour the boiling water.\"\n",
    "Return:\n",
    "{\n",
    "    \"queue\": [\n",
    "      {\"task_id\": \"1\", \"description\": \"Take the mug out.\"},\n",
    "      {\"task_id\": \"2\", \"description\": \"Add the tea bag.\"},\n",
    "      {\"task_id\": \"3\", \"description\": \"Pour the boiling water.\"}\n",
    "    ],\n",
    "    \"status\": \"SUCCESS\"\n",
    "}\n",
    "\n",
    "Example instruction:\n",
    "    \"The sky is blue and I am very happy\".\n",
    "Return:\n",
    "{\n",
    "    \"tasks\": [],\n",
    "    \"status\": \"FAILURE\"\n",
    "}\n",
    "\"\"\"\n",
    "structured_llm = test_structured_llm.with_structured_output(Tasks, method=\"json_mode\")\n",
    "SYSTEM_MESSAGE = SystemMessage(content=PROMPT)\n",
    "human_message = HumanMessage(content=\"Add 3 and 4, multiply the output by 2 and divide the output by 5.\")\n",
    "#structured_llm.invoke(\"Split the following sentence in tasks: 'Add 3 and 4. Multiply the output by 2. Divide the output by 5.'\")\n",
    "structured_llm.invoke([SYSTEM_MESSAGE] + [human_message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d1c29a84-140e-49b1-9a1f-fd3abba4ed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queue=[Task(task_id='1', description='Add 3 and 4.'), Task(task_id='2', description='Multiply by 2.')] status='SUCCESS'\n"
     ]
    }
   ],
   "source": [
    "message = AIMessage(content='{\"queue\":[{\"task_id\": \"1\", \"description\": \"Add 3 and 4.\"},{\"task_id\": \"2\", \"description\": \"Multiply by 2.\"}],\"status\":\"SUCCESS\"}')\n",
    "parser = PydanticOutputParser(pydantic_object=Tasks)\n",
    "parsed_tasks = parser.invoke(message)\n",
    "print(parsed_tasks)\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.messages import AnyMessage, ToolMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "class TasksMessageState(TypedDict):\n",
    "    tasks: Tasks\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    tools_called: int = 0\n",
    "    \n",
    "def steps_manager(state: TasksMessageState):\n",
    "   printmd(\"**=> Entering steps_manager node**\") # Debugging print\n",
    "   printmd(\"**=> Steps Manager State**\") # Debugging print\n",
    "   print(id(state))\n",
    "   for m in state['messages']:\n",
    "       m.pretty_print()\n",
    "   aimessage = structured_llm.invoke([SYSTEM_MESSAGE] + state[\"messages\"], stream=False)\n",
    "   #aimessage = parsed_tasks\n",
    "   printmd(\"**=> Steps Manager Result:**\")\n",
    "   print(aimessage) # Debugging print\n",
    "   #humman_message = HumanMessage(content=\"Process each task individually\")\n",
    "   return TasksMessageState(messages=state[\"messages\"], tasks=aimessage, tools_called=0)\n",
    "\n",
    "def assistant(state: TasksMessageState):\n",
    "   printmd(\"**=> Entering assistant node**\") # Debugging print\n",
    "   printmd(\"**=> Assistant State:**\") # Debugging print\n",
    "   print(state[\"tasks\"])\n",
    "   print(state[\"tools_called\"])\n",
    "   print(id(state))\n",
    "   print(id(state[\"tasks\"]))\n",
    "   for m in state['messages']:\n",
    "       m.pretty_print()\n",
    "       print(type(m))\n",
    "   tasks = state[\"tasks\"]\n",
    "   if tasks.status != \"SUCCESS\":\n",
    "       return {\"messages\": AIMessage(content=\"Unfortunately I am unable to process your request.\")}\n",
    "   elif tasks.queue:\n",
    "       task = tasks.queue.pop(0)\n",
    "       # If previous message is a ToolMessage (recurrent call), then\n",
    "       # get the value of the tool and use it\n",
    "       if isinstance(state['messages'][-1], ToolMessage):\n",
    "           last_value = str(state['messages'][-1].content)\n",
    "           human_message = HumanMessage(content=f\"You are evaluating a task that is using a previous result with value '{last_value}': '{task.description}'\")\n",
    "       else:\n",
    "           human_message = HumanMessage(content=f\"Evaluate task: '{task.description}'\")\n",
    "       human_message.pretty_print()\n",
    "       result = llm_with_tools.invoke([human_message], stream=False)\n",
    "       printmd(\"**=> Tool Result:**\") # Debugging print\n",
    "       print(type(result))\n",
    "       print(result)\n",
    "       #return {\"messages\": state['messages'] + [human_message] + [result]}\n",
    "       # It's not necessary to pass task. Apparently, if you don't pass the field it will show up when\n",
    "       # assistant gets called back. Still I am not sure how this works.\n",
    "       return {\"messages\": [result], \"tasks\": tasks, \"tools_called\": state[\"tools_called\"] + 1}  \n",
    "   else:\n",
    "       if isinstance(state['messages'][-1], ToolMessage):\n",
    "           last_value = str(state['messages'][-1].content)\n",
    "           return {\"messages\": AIMessage(content=f\"The result is {last_value}\")}\n",
    "       else:\n",
    "           # If the queue is empty but the last message wasn't\n",
    "           # a ToolMessage, then \"steps_manager\" returned an empty queue\n",
    "           # with status \"SUCCESS\".\n",
    "           return {\"messages\": \"Nothing to process\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c046dd11-9c68-431c-ae4c-819fbf2af8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tasks(queue=[Task(task_id='1', description='Add 3 and 4.'), Task(task_id='2', description='Multiply by 2.')], status='SUCCESS')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aef13cd4-05a6-4084-a620-2e7b91d9a72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANYAAAFcCAIAAAA73ddzAAAAAXNSR0IArs4c6QAAIABJREFUeJzt3XdcE/f/B/DPJSGDhABhD9myFBUEtaLigFpRqVDFhXW2Uke1alurtFVbqd/aWmfrrFpBqVuxKu5dUKlUEVFZIpsESEgg+35/xB/SGHAl97mQz/PhH3C53OcNvri7fO5zn8NwHAcIAg8FdgGIqUMRRCBDEUQgQxFEIEMRRCBDEUQgo8Eu4E2I6hQigaJJpJI0KpVy4+hWoplhVBpmbkE159JsnOhMcyrsisgCM47/QAAAALXl0sJ/JcX3JWwuTaXEzblUtgWNzqIAY/gJaAxMXK9salQ1iZQSoYptSfXsyu7cg8OxNoNdGmTGEUGhQHHjOJ9qhlnb0z27sG1dGLArelvlhc3FuZK6KpmVHb3vSBuamemeERlBBLNOCR7ebuwbY+vTnQO7Fv3790rDjXRB/1jbrn0tYdcCB9kjeHBdWddwrn8oF3YhhnUzo66xTjFkvAPsQiAgbwRxHN/6VVHMTGcnTxbsWoiQlyUquS+JnuYEuxCikTeCv31R8GGSB5trlJ/Z30z+LVHuDdHoea6wCyEUSSN4cG1Z+CgbJw+T2P+1du+6UFAhGzjGHnYhxCHjB7HMk4Kg/lwTzB8AICjc0tyC+uCmCHYhxCFdBOtr5AU5Yr+eHfzzRztChlhfOlALuwrikC6CN9IFfUfawK4CJpoZpWekddYpAexCCEKuCFaVSBksildQB+z/ey29hvKqSqQKuRp2IUQgVwQL74p5jnTCmsvNzZXJZLDe3j4mm1qcKzHQxkmFXBEsvi/x7MImpq309PQpU6Y0NzdDeftLeXZlowgSrb5GzuXRrB0I2gu+8Q5M041luP2fhlcQWyhQGLQJkiBRBIV8BYZhhtjykydPEhMT+/XrFx0dnZycrFar09PTV61aBQCIjIwMDQ1NT08HAOTk5MyZM6dfv379+vWbOXPmgwcPNG9vaGgIDQ3ds2dPUlJSv379PvroI51v1y+aGUXcoJQIlXrfMtmQ6NpDk0hlzjXIKLrvvvuupKRk4cKFEonk9u3bFAolPDw8ISEhJSVl7dq1HA7Hzc0NAFBRUSGTyWbMmEGhUA4cOPDpp5+mp6czmUzNRnbs2DFmzJjNmzdTqVQHB4cX3653bC5NIlKyLUn0f2QIJPrxJCKlgS7HVVRU+Pv7x8bGAgASEhIAADwez9XVFQDQtWtXKysrzWrDhg2Ljo7WfB0YGJiYmJiTk9OnTx/NkqCgoNmzZ7ds88W36x3bkioRqkAnA22eLEgUQQBwGsMgB+Lo6Ohdu3b9+OOPM2bM4PF4ba2GYdjFixdTUlKKi4vNzc0BAALB8865Xr16GaK2djCYVFxNxsun+kWic0EWm9ZYZ5BTn9mzZy9YsODMmTMxMTH79+9va7Xt27d//vnngYGBa9asmT9/PgBArX7eM8diEX3BsIEvNzeBURokiqA5l9okUhliyxiGTZgw4dixYxERET/++GNOTk7LSy2jNGQy2c6dO0eNGrVw4cIePXoEBQW9ypYNOsjDcCfHpEKiCFrwzMwMcyDWdKCw2ezExEQAQH5+fsterbb22dXY5uZmmUwWEBCg+bahoUFrL6hF6+2GYMGjWVh1/L0giX5COxdGeUGzuEHJ0ffv/csvv+RwOH369Ll27RoAQJOz7t27U6nUn376KSYmRiaTffDBBz4+PmlpaTY2NmKxeOvWrRQKpaCgoK1tvvh2/dZckicxo1MwikH+JkmFumzZMtg1PNdQq1BI1fZuTP1utqys7Nq1a6dPn25ubp47d+7AgQMBAFwu18HB4ezZs1evXhWJRCNGjAgJCbl+/fr+/fufPHkyd+5cd3f3Q4cOTZw4UaFQ/PHHH/369QsMDGzZ5otv12/Ndy42uPiw7Dvp+VdBQuQaslqaLynKlQwcbUIDNtuSvrViULwdx6rj3+JJogMxAMDNn511qq7qidTRXfdff0NDw6hRo3S+5OrqWlZW9uLyiIiI5cuX67tSbTNmzNB51A4ICGi5ytJaz549f/7557a2lntDyLGimUL+SLcXBACUFzRnnRbEzdF9/4RKpaqurtb5Eobp/llYLJa1tbW+y9RWW1urUOi4pNtWVQwGw8amzWGRW78qmvyNO4PV8T8OkzGCAICL+2s6B3NcO5vDLgSOe9eFcqm65xCD/9mQBIk6ZVoMirc/vbuqWWyQPkKSK33YVHRXbDr5I2kEAQDjv3Db+79S2FUQrbFecTal+v1PXGAXQigyHog1ZM2q1FWlExe7mcgpUfUT6ZmU6olfuVFMoC+wNfJGULNX2Pfj05iZTo4d/YbOh9mif68I4z/r6KNidCF1BDXO76tulqjCR9oSNqCaSGWPm66nC1x9WOExtrBrgcMIIggAKM6VXE/newWxHdyYnl3ZHeBQJZWoiu9LKoulQr4ifKSN3i8IGRHjiKDG4zuNj++Ii3MlAb25NDrG5tLYllQGk2oUPwCViklEyiaRUixUiuqU1U+knl3Yvj0t3PxMtO+phTFFsEXJA4mwRiERKSVClVKpVuu190ahUOTl5XXv3l2fGwWAxaHiatycS+NY0myc6M7eHfzs9tUZZQQNSiAQjB8//syZM7ALMRUk7RdETAeKIAIZiqA2DMN8fX1hV2FCUAS14Tj+6NEj2FWYEBRBbRiGWVqa6OT3UKAIasNxXCgUwq7ChKAI6uDo6Ai7BBOCIqhDVVUV7BJMCIqgNgzDWt8phxgaiqA2HMfz8vJgV2FCUAQRyFAEtWEY1s7sW4jeoQhqw3G8rq4OdhUmBEVQB1tbEx3ADAWKoA58Ph92CSYERRCBDEVQG4Zh3t7esKswISiC2nAcLywshF2FCUERRCBDEdShZbpfhAAogjronBEQMRAUQQQyFEFtaKQMwVAEtaGRMgRDEUQgQxHUhm7iJBiKoDZ0EyfBUAQRyFAEtaH7iAmGIqgN3UdMMBRBbWikDMFQBLWhkTIEQxFEIEMR1MHBwQF2CSYERVCHtp60iBgCiqAOaLwgkVAEdUDjBYmEIqgNDdYiGIqgNjRYi2Aogjq4uup+JjxiCOjRN89Mnz69qqqKSqWq1er6+noej4dhmFKpPHnyJOzSOji0F3wmPj6+sbGxoqKiqqpKJpNVVlZWVFRgmNE/b5H8UASfGTp0qJeXV+slOI737NkTXkWmAkXwufHjx5ubP38upqOj44QJE6BWZBJQBJ8bOnSou7u75mvNLtDf3x92UR0fiuB/fPjhh2w2W7MLHD9+POxyTAKK4H9ERUW5u7vjOB4cHIwu0xGDpvctyppV/HK5TKrW+5aJMerdmaDp6HsDJhflSmDX8mZwcwsaz5FOZxjH/kXP/YIZf1SV5DW5eLPUxppAo4djuLRR1dSo7Bxs0e99I5ixWG8RVMrVh9aXd+1v7ebP0csGkbd091qdpF7+bgLZH2amtwjuX/M0dKidnStTL1tD9OL+3/VNQsXgsfawC2mPfk4XHt9ptHVlovyRTZd3rMUNSkGFDHYh7dFPBGvLZEy2/j/ZIG+PakYRVMlhV9Ee/URQ1qzm2pjpZVOIfvEcGOIGJewq2qO3CKpJ/WOaLoVcrVKSejCUcXQdIR0YiiACGYogAhmKIAIZiiACGYogAhmKIAIZiiACGYogAhmKIAIZiiACGbkiWFVVWVlVAbsKhFAkimB5RdmEhJiHD9GUQqaFRBFUKZWmOcFNWVnpa63fwX5LcMaZSqXStetX3bhxBQDQrVvwnFmLcIBPnjoaALB8xeLlAAwdOmLxF8s0a27fsen8hdNyuayTq3t8/KTBg94FABw8tHfTr2vi4sZdvnxOLG4MDAiaOXOen28AACAz89rW7RsqKsocHZ1jRo6Oix3bTiVJ3yx06+QhlUnPnDmB43hIcK8P4sanpO7Ivf8vz9pm6pTEqKhoAEBNTfWOnb9mZV2XSMSdOrlPGD81csh7AIDHBQ/nfjptVfL6rds3FBY+cnBwmvnRp+HhEe28BQAgEPA3bFydnZ1FMzPr2bP3lSvnt/yW4unpDQA4dvzg/gMpfH6No6PzkMHvjY2fxGAwhMKGUXGRiTPnPS54eP36pfDwgUlLvifq/8rg4ERw776dGRknpk5JtLGxzThzgsVisVjmS5d8vzI5aeqUxOAeodbWPACAWq1emvRZVVXFxAlTrax4OTm3v/t+iVTaHD3sfc12FHL5d8t/quXX7Nq9ZcHCmdu3pVlyrZat+NLD3WvhgqTi4gKBoPalxexL2x0bO3bNz1syM6/t3LU5M+varE8WTJ8+e9++Xat+XObnF+jm5qFUKfPz778fM9qSa3Xl2oWVyUkuLp0C/LsAAGQy2fLvFs+d87mTo/POXZu/T16atveEpaVVW29RqVRLls6vqxfMm7e4ro6/bfvG4B6hmvzt2r31wMGUuNhx7u5eT5+W/Ln/j7Ly0iWLV2jqTEnZ8f77Y37+aTOLyTLw/w+h4ESwsqqCxWJNGD+FRqMNjx6lWejb2R8A4ObmERTUQ7PkytULd+/d2ZeabmtrBwCIHPJec3PTocP7WiKYOHO+ubl5AAB+voEJH446cuTP998fI5PJ+vcfHBU57BWLcXf3/HTO55oCTp466u/XJXZUPABg9qyFV69dzPk3283Nw9nJZdfvBzQTbQ0b9n7sB5HXr1/SRBAAMHfO55p984wZc2YmJvx7958B/Qe39ZYHD3IfPc7/9ptVAyMiAQClpSWnTh+Xy+UikTB17+9JS1dGDBii2ayNjd0va3+YM3uR5tvAwKAZ02fr9f+BFOBEMHLIsPPnT3+5eO7sWQu9vHzaWi0z85pSqZyQENOyRKVSsdk6bhJ1cHB0c/N4kJ/7SeL8Ll26paTuYDJZI0fE0en0lxbDoDNavqbTGTSzZ3cg2Ns7AACEwgbNtwWFj3bt3qL5tKRSqerqBC3vatktOTg4AQD4/Np23lJTWw0AcHZ+Noumq6ubWq1ubm7Kzs5SKpUrk5NWJidpXtKc8/Fra2xsbAEAISG9XvqzGCM4Eezdq+8Pyes2b1k7/aNxw6NHzZ+3mEbTUUl9vcDGxnbNT5tbL6TqWhMAYGHBbWwUYRi2Knn99h0bN29Ze+BgyldfrujePeTNitTswDQ5+OfOrS8Xzw3uEfrF59+yzdnfLPtcjeu4Wd+MZgYAUKtV7bzFxaUTAODevRzNXv/Bg1xbWztLSytBHR8AkLxyrb3df5564uzsKpGIAQDMjnX8bQHttrfevfqGhfY5dHjfr7/94uDgNClh+ovrWFhwGxrqHRycGAyGrm38B7+2ppObBwCAw+HMn7c4Pn7S198sTPp6wZ9pJ1tP2fZm9uzZ7uzsmrxyreZP5VXOxtp6i59vQFhon63b1ldXVzYI66/fuJy0dKXmh9Ws4Obm8ZbVGhc4nTJyuRwAQKFQxoyeaGtr9/hxPgCAwWACAAT85x8gQkJ6qVSq4+kHW5Y0Nzfr3GBOTnZ5RVmXwG6azwcAAGcnl7jYcWKJuEoffd1CUYOPt68mTHK5vKm5Sf2yKUvaecvcOZ+7uro9LXtiZWm9ccNOzUlhcHAYhmFHjv750h+2g4GzFzx8JO36jctRkdECQS2fX+vnF6g593J2ctl/MIXJYolEwrjYcVGR0eknDm/esq6yqsK3s39BwaNr1y/u+v0gk/nsnvlf1ib37Nm7oqLs0OF9PJ5N7KixCoVi8tQPBkZEeXp4Hzt2gMPmtJx1vY0ePUIzMtJPnjrGtbA8cCi1sVFUUlzYfv9cW29RqVSz5kweMzrBxaUThmGNjSKxWMzhcFxdOsXFjjt0eN+SpM/6hQ8UCPhHj+3/IXmd5njdgcGJoLOzq0Iu/23zL2w2Jy5u3Nj4SZpzr6Sk5B9XL9+46Sd7e8dBA991dHRa/b9N27ZvuHAh48SJw66ubjEjR7c+a1QqlZu3rJPLZd279/xk5nw2my1qFAX3CDt3/pREIvb09EleubYlr29j2pRP6gT8DRtXW1hwRwyPix+dsGZt8p2c2y1Hz1d/S0hwWGjPPntStiuVz+57teBYrF+3w8PDa/asBfb2DkeO/Hnr1t82Nrb9+w2ysyX1XBx6oZ85ZU7vrnL25ngGETehkaZr+q/0K29/nkc8lUpFpVI1n3UqKstnfDQufkzC1CmJhmjrzgUBi42FvcszxMb1ouPPwpGZeW3lD0k6X9q4fqe7uyfB9chksllzJtvbO3bvFmJmRr93745UKvX29iW4DPLo+BHs0SN065a9Ol+CcpjDMOzdqOEXLmTs3LWZTqd7evp8+82qAf0HE18JSRjrgRh5ReQ/EJNopAximlAEEchQBBHIUAQRyFAEEchQBBHIUAQRyFAEEchQBBHIUAQRyPRzjZhjRcVQmEmJRqcwzEn9f6Of4tiWZjVPTWKIr9GpLGqytif1I2H0E0E3P5aE3M9XMU0qFa5S4i7epL7vST8RtHFiuPmzrh6u0svWEH05u6e8TzSPQsVgF9IefT6P+P7fovzbjZ5dLWxdmHQmqc8/OjaJSNFQK79zoW7EDCdHd7I/m1LPj8SuLGm+/7dIXK9sqFXocbNEwnFcLpe/ym2j5IRhGMuC6uTJ7DnEytzCCIYk6zmCHYBAIBg/fvyZM2dgF2Iq0OESgQxFEIEMRVAbhmGBgYGwqzAhKILacBzPy0OTDRMHRVAbhmHe3t6wqzAhKILacBwvLCyEXYUJQRHUwc/PD3YJJgRFUIeHDx/CLsGEoAhqQ+eCBEMR1IbOBQmGIohAhiKoDcMwH582HwKA6B2KoDYcxwsKCmBXYUJQBBHIUAS1YRiml+mpkVeEIqgNx3GpVAq7ChOCIqgNwzAut8159BG9QxHUhuO4SCSCXYUJQRFEIEMR1MHFxQV2CSYERVCH8vJy2CWYEBRBBDIUQW1opAzBUAS1oZEyBEMRRCBDEdSGbuIkGIqgNnQTJ8FQBBHIUAS1oU/EBEMR1IY+ERMMRVAbhmHW1tawqzAhKILacByvr6+HXYUJQRFEIEMR1IZhmK+vL+wqTAiKoDYcxx89egS7ChOCIqhDQEAA7BJMCIqgDg8ePIBdgglBEdQBTe5GJBRBHdDkbkRCEdQBnQsSCT365plZs2YJhUIajSaXy4uLi729vWk0mkKh2Lt3L+zSOjgjeEAUMcLDw9evX69SqTTfomMxYdCB+JmxY8e+eO9mnz59IJVjQlAEn6HRaPHx8VQqtWUJl8udNGkS1KJMAorgc6NHj3Z2dtZ8jeO4n59f7969YRfV8aEIPkej0caMGaPZEVpaWk6ePBl2RSYBRfA/xowZ4+LiotkFohNBYpD3E7FIoMAoxD/RHnt/+LhDhw5NHDujsV5JeOsAAGBhTd7/FEMgXb+goEJ262x90T2xs495Q7UcdjlEs3VhlBc0de5h0S/Wls4wiWMUuSJY9UR6NrV6wGhHS1s6lUr8LpAU5DJ1XaXsXGr5lG88WRzqK7zDuJEogtWl0rOpNe/PcoNdCFn8sbzgk5+8KRDORghFol397bP1g8c7wa6CRAaPd7p2lA+7CoMjSwQVcnXpwyYLazPYhZCIpS295L4EdhUGR5YINtQo3APYsKsgFwueGcfKTCEny5mSgZAlgjgOhHwF7CpIp7pU2tFPBUkTQcRkoQgikKEIIpChCCKQoQgikKEIIpChCCKQoQgikKEIIpChCCKQoQgikJl0BE+eOjYqLrK6uqqtFVQq1b17OW/fUFVVZWVVxdtvp0My6QjS6Qw2m0OhtPlLWP3zd2vWJr9lK+UVZRMSYh4+RI/T0c207pTREjnkvcgh77Wzglwme/tWVEoleYamk5ARR/DevZw9Kdvv5eYAAPz9uiQmzvfzDQAASKXStetX3bhxBQDQrVvwnFmLHB2dMjOvbd2+oaKizNHROWbk6LjYsat+XJaRcQIAcDYjk0aj6Vzh4qWzAIBBQ0IBAHtTjzs5Op86ffzo0f1FxQUslnmvsHfmzF5kZWUNADh4aO+Fi2fGjJ64Y8cmQR2/c2f/RQuS3Nw8KqsqJk8dDQBYvmLxcgCGDh2x+ItlsH9z5GLEEayqqpDJZZMSZlAolGPHDiz+6tN9qelMJnPvvp0ZGSemTkm0sbHNOHOCxWI1NTUtW/Glh7vXwgVJxcUFAkEtACAudpxarT579iQAQOcKCROm1dZUV1aWf7V4BQDAhmcLAMjLu+fm5hEVFV1fX3f4SJqkSfLDyrWaeh48yN2/f8/ChUlKpXLNmpU//O/b3zbttuHZLl3y/crkpKlTEoN7hFpb82D/2kjHiCMYGTksKipa87WfX+CChYn3cnPCQvtUVlWwWKwJ46fQaLTh0aM0Z2Mymax//8FRkcNa3u7b2d/D3UvzdX1D3YsruLq6WVpa1dULgoJ6tCxc8NkSDHs2iJRGo6Wk/i6TyRgMhmbJyu9/4fFsAABxceN+/e0XoUhoybX07ewPAHBz82i9HaSFEUcQw7Cr1y7uP5Dy5Emxubk5AKC+TgAAiBwy7Pz5018unjt71kIvLx8AgLOTS5cu3VJSdzCZrJEj4uh0utamXrpCC4VCcfhI2tlzJ2tqqhgMplqtbmiod3Bw1LzKZLI0Xzg4OAEABPxaS66lIX8HHYERfyL+Y8/2b7793M83cOV3axJnzgcAqHE1AKB3r74/JK+rqxdM/2jcTz9/r1QqMQxblbx+6LsjNm9Z++GUuH///UdrUy9dQQPH8SVL56fu/X3YezH/W7UxKjK6pVEtZjQzAIBKrTLMj96hGGsEFQrF3n07h0ePmjN7YVBQj8CAoNav9u7Vd8e2tFmffPbXyaP70nYDADgczvx5i3fvOsRmc5K+XtDU1KS1wbZWaP1h9t9//8n+5+a8TxeP/mBCYEBXL08fQn7WDs5YIyiXy2Uyma/vs0mhhaIGAIBarda8BACgUChjRk+0tbV7/DgfACCTyTQH3LjYcWKJuOqFjmKdKzCZrLo6gWazLa1ozu20Gm0Hg8HUHJQN8GvoCIz1XJDNZnt5+Rw+ksbj2UjE4t1/bKVQKEVFBQCAw0fSrt+4HBUZLRDU8vm1fn6BCoVi8tQPBkZEeXp4Hzt2gMPmODu7tt5aWyt07xZy6vTxNb8kB3XtYWHBDQwIotPp27ZvHD48tqjo8d59OwEAxUUFLv/dmhZ7ewdnJ5f9B1OYLJZIJBwbP6mdznATZMS/i6+XJrOYrBXfffXngT2ffPLZpITpGRnpCoXC2dlVIZf/tvmXv04ejYsbNzZ+UrO0ObhH2Lnzp9auX0UzM0teuZbJZLbeVFsrREVFx46Kv3T57NbtG+7n3bWzs09auvJxQf6y5V9kZ2et+XlLnz79Dh9Ja79ODMOSkpLNzdkbN/10OiNds5NGWpBlTpmap7LzaTUjPu4EuxBySfm+8ONkL6pZR76X2Ij3gkjHgCKIQIYiiECGIohAhiKIQIYiiECGIohAhiKIQIYiiECGIohAhiKIQIYiiECGIohARp4I4tb2bd6xYbIc3JmkGMhkSGSJoI0To+heI+wqyEXIl0tESlqHHqlFoghSaZhnV05DrR5mL+gw6mvkXl07/uOAyBJBAECfaN751ErYVZCFrFl19VBVeIwt7EIMjiyjpjXqa+SHNpRFjHa0tKWzOMZ6X8tbEjco6qtklw5UfbTSy8wEHklMrggCACQiZdapuuJciZWdmaBS920WOAAqlYpG7QjP6lWqlDTq8z82B3dmQ43cuzun3/sdf/+nQboItpA2qbE2TsTj4+M3bdpkZ2dniHbT0tJSUlKWLFnSt29fQ2xfS3Z29u+//75p0ybNtxgAdFbH3/O1Rt4I6nT37t1u3boZbvtisXjKlCnFxcVhYWGbN282XEMvOn369HvvtTfTXEdlTH9wX3/9dWOjYTtuDh8+XFZWhmHY48ePr169atC2tLi5uUVERCiVSiIbJQPjiKBCoZBKpe+88054eLjhWpFIJMePH9eEQCgU7tmzx3BtvSgwMPCvv/6SSqUPHjwgsl3ojCCCOTk5u3fvZjAY0dHRBm3o4MGDT58+bfm2sLCQ4B0hh8PhcDgsFmvEiBGmc8e7EZwLzpw5c8uWLYZupampafLkycXFxS1L1Gp1WFgYAU2/qLKyEsdxFotlbW1NfOsEI/VeMDc3FwBATAgOHDhQWlraegmFQikoKCCg6Rc5OTk5OzvjOD5t2jSZPua7JjPyRnDatGmWlsTND5mVleXj49O5c+dOnTpRqVQ/P7/OnTtbWVkRVsCLeDzevHnz1qxZA7EGIuDko1Ao8vPzc3JyoLReU1Pz2WefQWm6Hdu2bYNdgqGQbi+Yn5+v2SF1794dSgEymaywsBBK0+3w8PBYsWIF7CoMglwRbGxs/O6778LDw6nwLr4pFIrOnTvDar0tkZGRH3/8seZPFHYtekaiCNbW1gqFwtTUVLhl8Pl8sVgMtwadHB0dNees27dvh12LPpElgqmpqWKx2NW1velKidHY2Ojr6wu7ijZNnjyZ/P1or4UUEaytra2urvb09IRdCAAAFBUVsVgs2FW056OPPtJcS4RdiH6QIoIYhi1YsAB2Fc80Nzf7+BjBVPqRkZGhoaEdYI8IOYL79u3bu3evrS2JxsZduXLF29sbdhUvx+Vyb9++LZPJ+Hw+7FreCswI3rx508nJacKECRBr0NLc3FxZWenl5QW7kFfFZDKzsrKysrJgF/LmYEawV69eAwcOhFjAi7Kzs3v16gW7itczfPjw3bt3w67izcGJ4D///DNnzhwoTbfvypUrBh0PZiC//vor7BLeHIQINjY2njt3buPGjcQ3/VLV1dUDBgyAXcUb+uabb+7cuQO7itcGIYIWFhZffPEF8e2+VGZmplKpNNAtKQRYsWJFdnZ2eXk57EJeD9ER3LFjB2nPnY8cORIbGwu7ircyY8YMFxcX2FW8HkIjeOPGDaFQ2Lt3byIbfUVisVgsFkdGRsIu5G1p7sCCXcVrMIJR08RYtWqVt7f3mDFjYBeiB/ktLgUpAAAPjUlEQVT5+ZcvX545cybsQl4JcRG8deuWhYWFv78/Mc29Fj6fP3HixIyMDNiFmCKCDsRisXjRokXkzB8AYOfOnYsWLYJdhZ4tX77cKC6cEBTBgoKCrVu3EtPW68rMzCwpKYmKioJdiJ7FxMQsXrwYdhUvh84FwdChQ1NTU0l1nVpfpFIplUo1MzODXUh7iNgL5ubmbtiwgYCG3sD69es//vjjDpk/AICZmVlVVRXsKl6CiAieOHFCM+KXbC5duvTkyZMPPvgAdiGGQqVSd+7ceezYMdiFtIeIA3FNTQ2Px6PRyDVfoFgsHj58+OXLl2EXYlh1dXVpaWmzZs2CXUibTPdccNy4ccnJyUY0LqujMviBuKSkhIT9HUuXLp0yZYqJ5C8/P5/MXZ4Gj2B5eTnZZuhJS0vr1KmT6Uzm5+3t/e2338Kuok0GPxDL5XKlUmlubm7QVl7dhQsXTp06tXr1atiFEOr69euenp7Ozs6wC9HBtM4Fc3NzV69ebdRjjDsegx+Ir1y5QpKJeWpra7dt22aa+auurl67di3sKnQzeASpVGpJSYmhW3mp5ubm2NjYdevWwS4EDgcHhwMHDkilUtiF6GDwA7FCoRCLxdCnagwLC8vKyqJQSHHfNBQlJSX29vbkOSlvYRLnghMnTty6dSub3fEfpmWMiNgrTJs2TSQSEdCQThEREVu2bEH5u3TpEjmv1BERQRaLlZeXFxMTExUVRXBv3MyZM8+cOcPhcIhslJwUCsXff/8NuwodDHjdduTIkVKptKGhQa1Wa25ZwnG8X79+hmtRS9++fS9evMhgMAhrkcx69+5tb28PuwodDLgXdHR0FAgEOI5j//8gLyqV2rNnT8O12EIul0+fPh3lrzUulwtr4tr2GTCC69at69SpU+slNjY2wcHBhmtRo7GxMSIiYseOHSh/rQkEgnnz5sGuQgcDRtDc3HzZsmWtd/5sNjsoKMhwLWr6YJcsWULOkx64qFSq5iEaZGPYjyPBwcGTJk1q6YsKDAw0aHMlJSVTp04l7QhtuEg7iYXBPxGPHz9+4MCBFAqFwWAY9Cb2vLy8hQsXnjx50nBNGDUqlTp06FDYVehARKfMihUr/P39eTxe165dDdTE/fv3f/jhh0OHDhlo+x2AUqmcP38+7Cp0eMnVkdpy2Z0LDdWl0max6m2awQGuVKrMDDZ2X6lS2jmbq1W4a2dWeEzHvBfpzSQmJt66dUvztVqtbrlEmZ2dDbWu59rLREme5Ea6oFsEL7CvNYtDrjs/XoRRgLBW3liv2LigYPoKTxYH2pNLSOXjjz8uKiqqq6vTHIs1Cx0cHGDX9Vybwcq/Jcq72Tgy0Y3Yet6KrQvT1oXp0YWTklw8aak7k41SCEJCQoKCglrfpYXjODG9s69I97mgtEmVl9UYlWBk04RpYBg2ZKLzlSO1sAshi4kTJ9rY2LR86+jomJCQALWi/9AdwcoiKZWGEV6M3ti5Mh/9I8bVHX8Q0KsICQnp0qWL5qRfswsk1bN9dEdQJFA4uJNuYNlr8e5uUVvWwZ/k++oSEhI0M0Y4ODhMnDgRdjn/oTuCMqlaKVcTXow+iQQKtXH/BPoUEhISEBCA43hYWJifnx/scv6D7J9zTZaoTt4kUjc1KmVNarlMD39M7/b+SF7r1L9b3L9XGt5+a3QGhcmmmltQ2ZY0jtVbpQhFkFyqSpof5zQV5UroLJpMoqIyqGZMMz2d1Nr1D51aWwpqS/VzW7dSplTKlUw2TSlT+nTn+HQ3t3NlvsF2UATJoqZMeumAQKnGzJgMB187pgUddkWvqlkkKy1qKsmvZzDxQWNsre1fr3IUQVI4/UdNRbHU3pvHsSH1Q0B1YnEZLC4DACCqkRzaUOkZZD4k/jUenGG6d5SRRLNYuW1psRxnefVyMcb8tca1Z/v0dW0U03eteKJ+5ZMHFEGYJI3K3d+VeoQ6c+2NuwusNSsnjmOA/a+LChXyVxpXgCIIjVAgT1td5j/Q3YzZ0U6HmBx61yjPnctK5dKXf5ZHEYQmddVTz15GeQn0FXmEOu9ZWfrS1VAE4Ti+tcqzpxOF2pF//3QWzd7P5tQfNe2v1pF/BaT1IEvUKMRZlh3/7ioLG/OaMkVxnqSddVAEIbieLrD34cGugiD2PtZXjwjaWQFFkGj3rjdYuXA73keQtrAsGEwu89GdxrZW0GcE8x7kymRvNTjl0uVzg4aElpbCnwzOcPKyxCzLN7mQRYAVP444eGyV3jfLtGA+uClu61W9RfB0RvrsOVOk0mZ9bbBDkjWr6qvlbGuSRtBALOzNnz5s83RQbxF8y/2fiSjOlVi7mNwcSxiG2XTilLTxoUQ/ZySnM9LXrlsFABgVFwkA+PKLb98bOhIAcObMX6n7dlZUlNnY2A6Pjp04YarmDi6lUrlz1+aMMyeEwgZ3d88pk2f2Cx/44mYzM69t3b6hoqLM0dE5ZuTouNixeqkWotpyOYVmqDtaCoqyT579taLqkQWH5+MZOizqE66FLQAgaeWQD0Z+mfvgUt7D6ywmp09Y7LuDZmjeolKpzl3akXn7qFze7O3VU6Ew1DSsGEYVVMo9AnVMsaefvWDvXuHxYxIAAD+sXLt+7fbevcIBABkZJ37437edO/t/nZQ8MCLq952/pe7dqVn/p5+//3P/nhHDY5cu+d7R0fnrbxbdvXtHa5tNTU3LVnxJN6MvXJDU950BAkFHuBdEIlTRGAaJ4OPCW9v++NTB3jN+1NIBfScUldzZvHO2XP4sUmmHlzs7+s6avjmk+7AzF7blPbyuWX7kxOqzl3b4+/aNHbGIbsZslrb5oeEt0ZhUcYNS90t6acDamufs7AoACAjoamlppblHYfvvm4KCeiQt+R4AMKD/4MZGUdqfuz+IG8/n12ScOfHhpBlTJs8EAEQMGJLwYeyu3VvW/Ly59TbrG+pkMln//oOjIofppUgyaGpUmlka5HLw0b9+7hMaGzvi2VOGfH16r14/9mFBZlDgQABAr5CYIRFTAADOjr43s489KsgM9Asvq8jPvH1kSMTUYZGJAIDQ4OGFxf8YojYAAI1OFQt1j1M0VNdAWVkpn187Nn5Sy5KwsHdOnjpWVl768GEeAKBfv0Ga5RiGhYX2OXtOeyIOZyeXLl26paTuYDJZI0fE0elGM36uHRQqZogrInX1ldW1xfy6p5m3j7Ze3iCs1nxBpz8bg0OlUi259kJRLQDgXt4lAMCAvuNb1scwQ3XSUWkYptZ9Q5yhIiiWiAEAVlbPO2AtLLgAAH5tjUQiBgBYt3qJy7VsamqSSP5zuoph2Krk9dt3bNy8Ze2Bgylffbmie/cQA1VLGDMGRSHVfTx6G41iAQAgatCMboGDWi+3sNAxsQSFQlOrVQCAhoYqJpPDNrfUez0vkktVVlzdEdRz6lumB7G3cwAACIXPb1Oor6/TBNHW1h4AIBIJW16qqxPQaDQmU7urgsPhzJ+3ePeuQ2w2J+nrBU1NTfqtlngcS6ry1YYwvRYW0wIAoFDI7O08Wv9jMdv79M1mW0ulYoWSiMezKWVKrrXu/Z3eIshisgAAfP6zDw02NraODk43b15vWeHy5XNMJtPHxy8goCuGYZlZ1zTL5XJ5Zta1Ll26UalUuhm9dTo1HT3OTi5xsePEEnFVVYW+qoWF52hmiEcc2Nm6WVk63vonXSZ/1i+rUimVSkX773J18QcA3LlLxBMSKRRgaaf74fDUZcuWvbi0vLBZpQSOHq8xiJfJMj92/EDJkyIMYHkP7vn5BVpwuH8eSKmtrVYoFIePpJ07f2rihGlhoX24FtyqqsojR/8EAOPza3/77ZfiksLPF33j5ORCMzM7cvTP/If33dw8bG3sPpwSx+fXCgT8I0f/lMtk06fNevWHGj++I/IIMH/Lm7v0js6g5Fyo43Xi6nezGIZZWzndzD6el38VB/iTp/eOnPhZpZK7dwoCAFy4+oers7+fz7OZ9TJvHWUy2cHd3rW39bx7/3z2nZPNUrFYUv/3rSOFxbddnQMC/fU/H3jZ3ZoBsXZmdB27PL1FkGvBtbNzuHTp7N9/X21sFA0dOsLHx9famnfh4plTp4831NdNmDA1YeI0zbzTYaHvSCTiU6ePXbiQwTZnL1qYFBb2DgDAgmPh5Oj8z51bFIwSEBhUVlZ67frFq9cu2NjYLf5imYuL66vXQ84IsjjUu1eELCsWja7nrhkHOw9Xl8CikpzsnJOlZfednHx69him6RdsK4IUCiXAt18t/8nd++eLSnIc7b3q6isc7Dz1HsGmBilQyoMHWel8Vffkbjcz6uRS0H2gEY/mOLmjLCLO1tGDdJfCMk8JKp5S9L4jJDN+Sb13IDU4QvcTuMi1kzAFIYOt7yQVtxPBRwU3//jzqxeXs5gWbXUdjxg6t0/oKH1V+ODh9dSD37y4HMdxAHCdHTeJUze5Ovvr3JpajdcUNIz+xKet5lAEiUZnULr1t6wobrDz1H1g8nDrtmDWnheX4zjA2phqypylz44Vb8+eOgtQq9U4jrdMUtga16LNuzZrC+v6jLBp61UUQTj6jrTZ+2M5jltiujJFpzN5dJjPrtZjAUqZCqgUIYPae+QOGrIKAYZhQ8bZltw2+j6mlyq+VT5s0ktua0cRhMPRnRk6xLI89yW39hi10juVg+LtuLYvubKKIghNULhl73e5ZXerYRdiEE/+qRwcb+PT/eWDI1EEYfLpzg6O4JTcKn/16S/ITylXFdx4Gj7CytXnlfqV0ccRyLr04dq7Ms6lVZqZs2w9IT+7/i3hOF5bVIepFPHzXbg2ui/HvQhFED47V8b4RZ2yTtXdPlfs2JnH5rGMaGY3jSahrEkorXpY13eETcjg13vkLIogWfQexguNss6+UP/wVq20WW3pxMEARmNQzVg0nX03cOFqtUKqUshUAOAN5Y1sS1pAGGf0zDb7n9uBIkgiVBrW611er3d5ojpF2eOm+mplY4NMJZNKhPof3/WWzDlUcybGcaDZONI7+XVic988SCiCZMTlmQX2JmIkKRnojiDNjKI2wLA2InEs0V+XcdDdKcO2pNZVGvd9wRWFTVZtjJFESEV3BG0c6Ub96KKmRqW9GxM9g84o6I6grQuDY0X790od4fXox5WDVT0G6h6HgpBNe88jvrC/lkLFukfwaGZGcxFF2qS8tL+65xBLr64mN2+GkXrJI7FvnanLvSGkmVFYFmQ/u+dY0soLmmydGT0GWroH6Jg4AiGnl0RQM+pVyFc0iUjXNfUCzMqe9jYdVAgUL48gghiU0ZzkIR0ViiACGYogAhmKIAIZiiACGYogAtn/Aa8MZZZTgcF3AAAAAElFTkSuQmCC",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Graph\n",
    "builder = StateGraph(TasksMessageState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"steps_manager\", steps_manager)\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"steps_manager\")\n",
    "builder.add_edge(\"steps_manager\", \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "react_graph = builder.compile()\n",
    "\n",
    "# Show\n",
    "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "75602459-d8ca-47b4-9518-3f38343ebfe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**=> Entering steps_manager node**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**=> Steps Manager State**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140056238077184\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 3 and 4. Multiply the output by 2. Divide the output by 5.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**=> Steps Manager Result:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queue=[Task(task_id='1', description='Multiply 3 and 4.'), Task(task_id='2', description='Multiply the output by 2.'), Task(task_id='3', description='Divide the output by 5.')] status='SUCCESS'\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**=> Entering assistant node**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**=> Assistant State:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queue=[Task(task_id='1', description='Multiply 3 and 4.'), Task(task_id='2', description='Multiply the output by 2.'), Task(task_id='3', description='Divide the output by 5.')] status='SUCCESS'\n",
      "0\n",
      "140056209221248\n",
      "140056210136400\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 3 and 4. Multiply the output by 2. Divide the output by 5.\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Evaluate task: 'Multiply 3 and 4.'\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**=> Tool Result:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.1:8b-instruct-q8_0', 'created_at': '2025-02-19T09:44:33.003245913Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6782898089, 'load_duration': 13435131, 'prompt_eval_count': 290, 'prompt_eval_duration': 1332000000, 'eval_count': 22, 'eval_duration': 5436000000, 'message': Message(role='assistant', content='', images=None, tool_calls=[ToolCall(function=Function(name='multiply', arguments={'a': 3, 'b': 4}))])} id='run-ff69358a-fbe9-47d0-8749-ecf28bf88d0b-0' tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 4}, 'id': '815fb200-7a28-4a50-aac0-56b135287b14', 'type': 'tool_call'}] usage_metadata={'input_tokens': 290, 'output_tokens': 22, 'total_tokens': 312}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**=> Entering assistant node**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**=> Assistant State:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queue=[Task(task_id='2', description='Multiply the output by 2.'), Task(task_id='3', description='Divide the output by 5.')] status='SUCCESS'\n",
      "1\n",
      "140056210087616\n",
      "140056210136400\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 3 and 4. Multiply the output by 2. Divide the output by 5.\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (815fb200-7a28-4a50-aac0-56b135287b14)\n",
      " Call ID: 815fb200-7a28-4a50-aac0-56b135287b14\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "12\n",
      "<class 'langchain_core.messages.tool.ToolMessage'>\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "You are evaluating a task that is using a previous result with value '12': 'Multiply the output by 2.'\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**=> Tool Result:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.1:8b-instruct-q8_0', 'created_at': '2025-02-19T09:44:40.89350908Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7883777746, 'load_duration': 11442017, 'prompt_eval_count': 303, 'prompt_eval_duration': 1526000000, 'eval_count': 22, 'eval_duration': 6345000000, 'message': Message(role='assistant', content='', images=None, tool_calls=[ToolCall(function=Function(name='multiply', arguments={'a': '12', 'b': '2'}))])} id='run-ad0bb71d-39f8-45e7-8946-b5ba23bc11c0-0' tool_calls=[{'name': 'multiply', 'args': {'a': 12, 'b': 2}, 'id': 'a111b165-1a97-4159-bb53-a35a0adeeb64', 'type': 'tool_call'}] usage_metadata={'input_tokens': 303, 'output_tokens': 22, 'total_tokens': 325}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**=> Entering assistant node**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**=> Assistant State:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queue=[Task(task_id='3', description='Divide the output by 5.')] status='SUCCESS'\n",
      "2\n",
      "140056209505344\n",
      "140056210136400\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 3 and 4. Multiply the output by 2. Divide the output by 5.\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (815fb200-7a28-4a50-aac0-56b135287b14)\n",
      " Call ID: 815fb200-7a28-4a50-aac0-56b135287b14\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "12\n",
      "<class 'langchain_core.messages.tool.ToolMessage'>\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (a111b165-1a97-4159-bb53-a35a0adeeb64)\n",
      " Call ID: a111b165-1a97-4159-bb53-a35a0adeeb64\n",
      "  Args:\n",
      "    a: 12\n",
      "    b: 2\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "24\n",
      "<class 'langchain_core.messages.tool.ToolMessage'>\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "You are evaluating a task that is using a previous result with value '24': 'Divide the output by 5.'\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**=> Tool Result:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.1:8b-instruct-q8_0', 'created_at': '2025-02-19T09:44:47.771220685Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6870977006, 'load_duration': 11518731, 'prompt_eval_count': 304, 'prompt_eval_duration': 867000000, 'eval_count': 22, 'eval_duration': 5991000000, 'message': Message(role='assistant', content='', images=None, tool_calls=[ToolCall(function=Function(name='divide', arguments={'a': '24', 'b': '5'}))])} id='run-85cf688f-465f-4703-a992-84f3eedd7d3f-0' tool_calls=[{'name': 'divide', 'args': {'a': 24, 'b': 5}, 'id': '3162d58d-8782-4e2f-87de-3a6713469c91', 'type': 'tool_call'}] usage_metadata={'input_tokens': 304, 'output_tokens': 22, 'total_tokens': 326}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**=> Entering assistant node**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**=> Assistant State:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queue=[] status='SUCCESS'\n",
      "3\n",
      "140056210926208\n",
      "140056210136400\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 3 and 4. Multiply the output by 2. Divide the output by 5.\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (815fb200-7a28-4a50-aac0-56b135287b14)\n",
      " Call ID: 815fb200-7a28-4a50-aac0-56b135287b14\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "12\n",
      "<class 'langchain_core.messages.tool.ToolMessage'>\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (a111b165-1a97-4159-bb53-a35a0adeeb64)\n",
      " Call ID: a111b165-1a97-4159-bb53-a35a0adeeb64\n",
      "  Args:\n",
      "    a: 12\n",
      "    b: 2\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "24\n",
      "<class 'langchain_core.messages.tool.ToolMessage'>\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  divide (3162d58d-8782-4e2f-87de-3a6713469c91)\n",
      " Call ID: 3162d58d-8782-4e2f-87de-3a6713469c91\n",
      "  Args:\n",
      "    a: 24\n",
      "    b: 5\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: divide\n",
      "\n",
      "4.8\n",
      "<class 'langchain_core.messages.tool.ToolMessage'>\n"
     ]
    }
   ],
   "source": [
    "#messages = [HumanMessage(content=\"Add 3 and 4. Multiply the output by 2.\")]\n",
    "messages = [HumanMessage(content=\"Multiply 3 and 4. Multiply the output by 2. Divide the output by 5.\")]\n",
    "#messages = [HumanMessage(content=\"Champaign!\")]\n",
    "#messages = [HumanMessage(content=\"Add 3 and 4. Execute previous result by function return_random_number. Divide the last output by 5.\")]\n",
    "messages = react_graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b517142d-c40c-48bf-a5b8-c8409427aa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queue=[] status='SUCCESS'\n",
      "3\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 3 and 4. Multiply the output by 2. Divide the output by 5.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (815fb200-7a28-4a50-aac0-56b135287b14)\n",
      " Call ID: 815fb200-7a28-4a50-aac0-56b135287b14\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "12\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (a111b165-1a97-4159-bb53-a35a0adeeb64)\n",
      " Call ID: a111b165-1a97-4159-bb53-a35a0adeeb64\n",
      "  Args:\n",
      "    a: 12\n",
      "    b: 2\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "24\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  divide (3162d58d-8782-4e2f-87de-3a6713469c91)\n",
      " Call ID: 3162d58d-8782-4e2f-87de-3a6713469c91\n",
      "  Args:\n",
      "    a: 24\n",
      "    b: 5\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: divide\n",
      "\n",
      "4.8\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result is 4.8\n"
     ]
    }
   ],
   "source": [
    "print(messages[\"tasks\"])\n",
    "print(messages[\"tools_called\"])\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "613c9cd5-1334-4e9b-96a0-5ff0c427bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = react_graph.invoke({\"messages\": \"You did it wrong, calculated all the tool values without knowin the results\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f35bfa07-5f95-46b1-9f53-49793acb84a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "You did it wrong, calculated all the tool values without knowin the results\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (653ae37c-e30a-46fa-81e6-a0751591749d)\n",
      " Call ID: 653ae37c-e30a-46fa-81e6-a0751591749d\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "  add (b1bccdf7-f6f2-4af3-bd77-b382e047fec1)\n",
      " Call ID: b1bccdf7-f6f2-4af3-bd77-b382e047fec1\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 12\n",
      "  multiply (17ddc6b1-95bb-42a6-9a44-0605bf637808)\n",
      " Call ID: 17ddc6b1-95bb-42a6-9a44-0605bf637808\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "12\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "14\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "12\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (9c0dfbde-00c4-41bf-9f2b-e4477ad893c0)\n",
      " Call ID: 9c0dfbde-00c4-41bf-9f2b-e4477ad893c0\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "  multiply (a0dfbc1c-cf3d-4ab8-81bf-e1130acc9eea)\n",
      " Call ID: a0dfbc1c-cf3d-4ab8-81bf-e1130acc9eea\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "12\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "12\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "To calculate the result of the original question, we need to follow the order of operations. First, we need to multiply 3 and 4, which is 12. Then, we need to add 2 and 12, which is 14. Finally, we need to multiply 3 and 4 again, but this time with the result of the previous operation, which is also 12.\n",
      "\n",
      "So, the final answer is:\n",
      "\n",
      "12 * (2 + 12) = 12 * 14 = 168\n"
     ]
    }
   ],
   "source": [
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad869f22-9bfb-4cbe-9f30-8a307c5cdda2",
   "metadata": {},
   "source": [
    "## LangSmith\n",
    "\n",
    "We can look at traces in LangSmith."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
